#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import glob
from pathlib import Path
from collections import defaultdict
from Bio import SeqIO  # ç”¨äºè§£æ FASTA æ–‡ä»¶

# ========== é…ç½®è·¯å¾„ ==========
input_gff    = "data/ninanjie/ninanjie1/ninanjie1.gff"
input_fna    = "data/ninanjie/ninanjie1/ninanjie1.fna"
input_base   = Path(input_gff).parent.parent
source_dir   = input_base / "output"
output_base  = input_base.parent / "ninanjieworkflow3"
splitgff_dir = output_base / "splitgff"
gffindex_dir = output_base / "gffindex"
annotated_dir = output_base / "sp18"  # æ–°å¢æ³¨é‡Šè¾“å‡ºç›®å½•
fa_pattern   = str(input_base) + "workflow2/*.fa"

# ========== ç¡®ä¿ç›®å½•å­˜åœ¨ ==========
output_base.mkdir(parents=True, exist_ok=True)
splitgff_dir.mkdir(parents=True, exist_ok=True)
gffindex_dir.mkdir(parents=True, exist_ok=True)
annotated_dir.mkdir(parents=True, exist_ok=True)

# ========== å·¥å…·å‡½æ•° ==========
def format_chr_name(text):
    return re.sub(r'chr(\d)(?=\D|$)', r'chr0\1', text)

def process_file(filepath):
    content = Path(filepath).read_text()
    Path(filepath).write_text(format_chr_name(content))

def unify_chr_numbers():
    print("ğŸ” ç»Ÿä¸€ chr ç¼–å·æ ¼å¼...")
    for bed_dir in glob.glob(str(output_base / "*.bed")):
        fa = Path(bed_dir) / "yuangene.fa"
        if fa.exists():
            process_file(str(fa))
        gff3 = Path(bed_dir) / "yuangene.gff3"
        if gff3.exists():
            process_file(str(gff3))
    print("âœ… chr ç¼–å·æ ¼å¼ç»Ÿä¸€å®Œæˆ")

def extract_prefix(filename):
    m = re.match(r'^([A-Za-z0-9]+)', filename)
    return m.group(1) if m else "split"

def split_and_rename_gff(file_path, output_dir, prefix=None):
    if prefix is None:
        prefix = extract_prefix(Path(file_path).name)
    lines = Path(file_path).read_text().splitlines()
    groups = defaultdict(list)
    for L in lines:
        if L.startswith('#'):
            continue
        chrom = L.split('\t',1)[0]
        groups[chrom].append(L + "\n")
    i = 1
    for chrom, seqs in groups.items():
        out = Path(output_dir) / f"{prefix}#1#chr{i}.gff"
        out.write_text("".join(seqs))
        i += 1

def process_output_gff_files():
    for root, _, files in os.walk(source_dir):
        for fn in files:
            if fn.endswith(('.gff','.gff3')):
                fp = Path(root) / fn
                split_and_rename_gff(str(fp), splitgff_dir)

def split_gff_by_c_blocks(gff_path, output_dir):
    """æŒ‰æ ‡å‡†æŸ“è‰²ä½“ï¼ˆchr+æ•°å­—ï¼‰æ‹†åˆ†GFFæ–‡ä»¶"""
    chr_groups = defaultdict(list)
    
    for line in Path(gff_path).read_text().splitlines():
        if line.startswith("#"):
            continue
        
        chrom = line.split('\t')[0]
        # ä»…å¤„ç†chrå¼€å¤´ä¸”å¸¦æ•°å­—çš„æŸ“è‰²ä½“ï¼ˆå¦‚chr1, chr02ï¼‰
        if re.match(r'chr\d+', chrom):
            chr_groups[chrom].append(line + "\n")
    
    # æŒ‰æŸ“è‰²ä½“åç§°æ’åºåè¾“å‡º
    for idx, (chrom, lines) in enumerate(
        sorted(chr_groups.items(), key=lambda x: int(re.search(r'\d+', x[0]).group())), 
        start=1
    ):
        bed_dir = Path(output_dir) / f"{idx}.bed"
        bed_dir.mkdir(parents=True, exist_ok=True)
        (bed_dir / "yuangene.gff3").write_text("".join(lines))

def split_fna_by_chromosomes(fna_path, output_dir):
    """åŒæ ·åªå¤„ç†æ ‡å‡†æŸ“è‰²ä½“çš„FNAæ–‡ä»¶"""
    chrom_records = {}
    for record in SeqIO.parse(fna_path, "fasta"):
        if re.match(r'chr\d+', record.id):
            chrom_num = int(re.search(r'\d+', record.id).group())
            chrom_records[chrom_num] = record
    
    # æŒ‰æŸ“è‰²ä½“ç¼–å·æ’åºè¾“å‡º
    for chrom_num in sorted(chrom_records.keys()):
        bed_dir = Path(output_dir) / f"{chrom_num}.bed"
        bed_dir.mkdir(parents=True, exist_ok=True)
        SeqIO.write(chrom_records[chrom_num], bed_dir / "yuangene.fa", "fasta")
def generate_gffindex():
    print("ğŸ” ç”Ÿæˆ gffindex...")
    for fa in glob.glob(fa_pattern):
        fn   = Path(fa).stem
        outp = gffindex_dir / f"{fn}.txt"
        with open(fa) as fin, open(outp,'w') as fout:
            for L in fin:
                if L.startswith('>'):
                    fout.write(L)
    print("âœ… gffindex ç”Ÿæˆå®Œæˆ")

# ========== æ–°å¢æ³¨é‡Šå¤„ç†åŠŸèƒ½ ==========
def load_annotations(index_file):
    """åŠ è½½æ³¨é‡Šæ–‡ä»¶å¹¶è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºå‰ç¼€ï¼Œå€¼ä¸ºæ³¨é‡Šè¡Œçš„åˆ—è¡¨"""
    annotations = {}
    with open(index_file, 'r') as f:
        for line in f:
            if line.startswith('>'):
                # æå–ç¬¬ä¸€ä¸ª#ä¹‹å‰çš„å†…å®¹ä½œä¸ºå‰ç¼€
                prefix = line.split('#')[0][1:]
                if prefix not in annotations:
                    annotations[prefix] = []
                annotations[prefix].append(line.strip())
    return annotations

def process_sp_file(sp_file, output_file, annotations):
    """å¤„ç†å•ä¸ª sp æ–‡ä»¶ï¼Œæ·»åŠ æ³¨é‡Šå¹¶è¾“å‡ºåˆ°æ–°æ–‡ä»¶"""
    prefix = sp_file.stem
    
    if prefix not in annotations:
        print(f"âš ï¸ è·³è¿‡ {sp_file}, æ— å¯¹åº”æ³¨é‡Š")
        return
    
    annotation_lines = annotations[prefix]
    annotation_index = 0
    prev_start_pos_length = None
    
    with open(sp_file, 'r') as infile, open(output_file, 'w') as outfile:
        for line in infile:
            parts = line.strip().split()
            if len(parts) < 4:
                continue
                
            # æå–åæ ‡ä¿¡æ¯
            chr_id = parts[0]
            node_id = parts[1]
            start_pos = parts[2]
            end_pos = parts[3]
            
            # æ£€æŸ¥èµ·å§‹ä½ç½®å­—ç¬¦ä¸²é•¿åº¦
            current_start_pos_length = len(start_pos)
            
            # å½“åæ ‡é•¿åº¦å˜å°æ—¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ³¨é‡Š
            if prev_start_pos_length is not None and current_start_pos_length < prev_start_pos_length:
                annotation_index += 1
                if annotation_index >= len(annotation_lines):
                    print(f"âš ï¸ {sp_file} çš„æ³¨é‡Šè¡Œä¸è¶³ï¼Œå·²ä½¿ç”¨æœ€åä¸€ä¸ªæ³¨é‡Š")
                    annotation_index = len(annotation_lines) - 1
            
            # æ›´æ–°å‰ä¸€ä¸ªä½ç½®é•¿åº¦
            prev_start_pos_length = current_start_pos_length
            
            # è·å–å½“å‰æ³¨é‡Š
            current_annotation = annotation_lines[annotation_index]
            
            # å†™å…¥æ–°è¡Œï¼ˆæ·»åŠ æ³¨é‡Šåˆ°ç¬¬5åˆ—ï¼‰
            outfile.write(f"{chr_id}\t{node_id}\t{start_pos}\t{end_pos}\t{current_annotation}\n")

def process_sp_files():
    """å¤„ç†æ‰€æœ‰spç›®å½•ä¸­çš„æ–‡ä»¶å¹¶æ·»åŠ æ³¨é‡Š"""
    print("\nğŸ” å¼€å§‹å¤„ç†æ³¨é‡Šæ·»åŠ ...")
    
    for i in range(100):  # å‡è®¾æœ€å¤š100ä¸ªå­ç›®å½•
        # è·å–ç´¢å¼•æ–‡ä»¶
        index_file = gffindex_dir / f"{i}all_genomes.txt"
        if not index_file.exists():
            continue
        
        # åŠ è½½æ³¨é‡Š
        annotations = load_annotations(index_file)
        
        # æŸ¥æ‰¾å¯¹åº”çš„spç›®å½•
        sp_dir = output_base / f"{i}.bed" / f"sp{i}"
        if not sp_dir.exists():
            continue
            
        # å¤„ç†æ¯ä¸ªbedæ–‡ä»¶
        for bed_file in sp_dir.glob("*.bed"):
            # åˆ›å»ºè¾“å‡ºç›®å½•
            out_dir = annotated_dir / f"{i}.bed"
            out_dir.mkdir(exist_ok=True)
            
            # å¤„ç†æ–‡ä»¶
            process_sp_file(bed_file, out_dir / bed_file.name, annotations)
            
            print(f"âœ… å·²å¤„ç†: {bed_file} â†’ {out_dir / bed_file.name}")
    
    print("ğŸ‰ æ³¨é‡Šæ·»åŠ å®Œæˆ")

# ========== ä¸»æµç¨‹ ==========
def main():
    # åŸæœ‰å¤„ç†æµç¨‹
    process_output_gff_files()
    split_gff_by_c_blocks(input_gff, output_base)
    split_fna_by_chromosomes(input_fna, output_base)
    unify_chr_numbers()
    generate_gffindex()
    
    # æ–°å¢æ³¨é‡Šå¤„ç†
    process_sp_files()
    
    print("\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()